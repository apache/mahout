/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements. See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied. See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
package org.apache.mahout.flinkbindings

import org.apache.flink.api.common.functions.MapFunction
import org.apache.flink.api.scala.DataSet
import org.apache.flink.api.scala.hadoop.mapreduce.HadoopOutputFormat
import org.apache.mahout.common.RandomUtils

import scala.collection.immutable.List

//import org.apache.flink.api.scala.hadoop.mapreduce.HadoopOutputFormat
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, SequenceFileOutputFormat}
import org.apache.mahout.math.scalabindings._
import org.apache.mahout.math._
import org.apache.mahout.math.drm._
import RLikeDrmOps._
import RLikeOps._
import math._

import org.apache.mahout.math.decompositions._
import org.scalatest.{FunSuite, Matchers}


import scala.reflect.ClassTag
import org.apache.flink.api.scala._



class FailingTestsSuite extends FunSuite with DistributedFlinkSuite with Matchers {

// // passing now
//  test("Simple DataSet to IntWritable") {
//    val path = TmpDir + "flinkOutput"
//
//    implicit val typeInfo = createTypeInformation[(Int,Int)]
//    val ds = env.fromElements[(Int,Int)]((1,2),(3,4),(5,6),(7,8))
//   // val job = new JobConf
//
//
//    val writableDataset : DataSet[(IntWritable,IntWritable)] =
//      ds.map( tuple =>
//        (new IntWritable(tuple._1.asInstanceOf[Int]), new IntWritable(tuple._2.asInstanceOf[Int]))
//    )
//
//    val job: Job = new Job()
//
//    job.setOutputKeyClass(classOf[IntWritable])
//    job.setOutputValueClass(classOf[IntWritable])
//
//    // setup sink for IntWritable
//    val sequenceFormat = new SequenceFileOutputFormat[IntWritable, IntWritable]
//    val hadoopOutput  = new HadoopOutputFormat[IntWritable,IntWritable](sequenceFormat, job)
//    FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(path))
//
//    writableDataset.output(hadoopOutput)
//
//    env.execute(s"dfsWrite($path)")
//
//  }


  test("C = A + B, identically partitioned") {

    val inCoreA = dense((1, 2, 3), (3, 4, 5), (5, 6, 7))

    val A = drmParallelize(inCoreA, numPartitions = 2)

     //   printf("A.nrow=%d.\n", A.rdd.count())

    // Create B which would be identically partitioned to A. mapBlock() by default will do the trick.
    val B = A.mapBlock() {
      case (keys, block) =>
        val bBlock = block.like() := { (r, c, v) => util.Random.nextDouble()}
        keys -> bBlock
    }
      // Prevent repeated computation non-determinism
      // flink problem is here... checkpoint is not doing what it should
      // ie. greate a physical plan w/o side effects
      .checkpoint()

    val inCoreB = B.collect

    printf("A=\n%s\n", inCoreA)
    printf("B=\n%s\n", inCoreB)

    val C = A + B

    val inCoreC = C.collect

    printf("C=\n%s\n", inCoreC)

    // Actual
    val inCoreCControl = inCoreA + inCoreB

    (inCoreC - inCoreCControl).norm should be < 1E-10
  }
//// Passing now.
//  test("C = inCoreA %*%: B") {
//
//    val inCoreA = dense((1, 2, 3), (3, 4, 5), (4, 5, 6), (5, 6, 7))
//    val inCoreB = dense((3, 5, 7, 10), (4, 6, 9, 10), (5, 6, 7, 7))
//
//    val B = drmParallelize(inCoreB, numPartitions = 2)
//    val C = inCoreA %*%: B
//
//    val inCoreC = C.collect
//    val inCoreCControl = inCoreA %*% inCoreB
//
//    println(inCoreC)
//    (inCoreC - inCoreCControl).norm should be < 1E-10
//
//  }

  test("dsqDist(X,Y)") {
    val m = 100
    val n = 300
    val d = 7
    val mxX = Matrices.symmetricUniformView(m, d, 12345).cloned -= 5
    val mxY = Matrices.symmetricUniformView(n, d, 1234).cloned += 10
    val (drmX, drmY) = (drmParallelize(mxX, 3), drmParallelize(mxY, 4))

    val mxDsq = dsqDist(drmX, drmY).collect
    val mxDsqControl = new DenseMatrix(m, n) := { (r, c, _) â‡’ (mxX(r, ::) - mxY(c, ::)) ^= 2 sum }
    (mxDsq - mxDsqControl).norm should be < 1e-7
  }

  test("dsqDist(X)") {
    val m = 100
    val d = 7
    val mxX = Matrices.symmetricUniformView(m, d, 12345).cloned -= 5
    val drmX = drmParallelize(mxX, 3)

    val mxDsq = dsqDist(drmX).collect
    val mxDsqControl = sqDist(drmX)
    (mxDsq - mxDsqControl).norm should be < 1e-7
  }

//// passing now
//  test("DRM DFS i/o (local)") {
//
//    val uploadPath = TmpDir + "UploadedDRM"
//
//    val inCoreA = dense((1, 2, 3), (3, 4, 5))
//    val drmA = drmParallelize(inCoreA)
//
//    drmA.dfsWrite(path = uploadPath)
//
//    println(inCoreA)
//
//    // Load back from hdfs
//    val drmB = drmDfsRead(path = uploadPath)
//
//    // Make sure keys are correctly identified as ints
//    drmB.checkpoint(CacheHint.NONE).keyClassTag shouldBe ClassTag.Int
//
//    // Collect back into in-core
//    val inCoreB = drmB.collect
//
//    // Print out to see what it is we collected:
//    println(inCoreB)
//
//    (inCoreA - inCoreB).norm should be < 1e-7
//  }



  test("dspca") {

    val rnd = RandomUtils.getRandom

    // Number of points
    val m = 500
    // Length of actual spectrum
    val spectrumLen = 40

    val spectrum = dvec((0 until spectrumLen).map(x => 300.0 * exp(-x) max 1e-3))
    printf("spectrum:%s\n", spectrum)

    val (u, _) = qr(new SparseRowMatrix(m, spectrumLen) :=
      ((r, c, v) => if (rnd.nextDouble() < 0.2) 0 else rnd.nextDouble() + 5.0))

    // PCA Rotation matrix -- should also be orthonormal.
    val (tr, _) = qr(Matrices.symmetricUniformView(spectrumLen, spectrumLen, rnd.nextInt) - 10.0)

    val input = (u %*%: diagv(spectrum)) %*% tr.t
    val drmInput = drmParallelize(m = input, numPartitions = 2)

    // Calculate just first 10 principal factors and reduce dimensionality.
    // Since we assert just validity of the s-pca, not stochastic error, we bump p parameter to
    // ensure to zero stochastic error and assert only functional correctness of the method's pca-
    // specific additions.
    val k = 10

    // Calculate just first 10 principal factors and reduce dimensionality.
    var (drmPCA, _, s) = dspca(drmA = drmInput, k = 10, p = spectrumLen, q = 1)
    // Un-normalized pca data:
    drmPCA = drmPCA %*% diagv(s)

    val pca = drmPCA.checkpoint(CacheHint.NONE).collect

    // Of course, once we calculated the pca, the spectrum is going to be different since our originally
    // generated input was not centered. So here, we'd just brute-solve pca to verify
    val xi = input.colMeans()
    for (r <- 0 until input.nrow) input(r, ::) -= xi
    var (pcaControl, _, sControl) = svd(m = input)
    pcaControl = (pcaControl %*%: diagv(sControl))(::, 0 until k)

    printf("pca:\n%s\n", pca(0 until 10, 0 until 10))
    printf("pcaControl:\n%s\n", pcaControl(0 until 10, 0 until 10))

    (pca(0 until 10, 0 until 10).norm - pcaControl(0 until 10, 0 until 10).norm).abs should be < 1E-5

  }

  test("dals") {

    val rnd = RandomUtils.getRandom

    // Number of points
    val m = 500
    val n = 500

    // Length of actual spectrum
    val spectrumLen = 40

    // Create singluar values with decay
    val spectrum = dvec((0 until spectrumLen).map(x => 300.0 * exp(-x) max 1e-3))
    printf("spectrum:%s\n", spectrum)

    // Create A as an ideal input
    val inCoreA = (qr(Matrices.symmetricUniformView(m, spectrumLen, 1234))._1 %*%: diagv(spectrum)) %*%
      qr(Matrices.symmetricUniformView(n, spectrumLen, 2345))._1.t
    val drmA = drmParallelize(inCoreA, numPartitions = 2)

    // Decompose using ALS
    val (drmU, drmV, rmse) = dals(drmA = drmA, k = 20).toTuple
    val inCoreU = drmU.collect
    val inCoreV = drmV.collect

    val predict = inCoreU %*% inCoreV.t

    printf("Control block:\n%s\n", inCoreA(0 until 3, 0 until 3))
    printf("ALS factorized approximation block:\n%s\n", predict(0 until 3, 0 until 3))

    val err = (inCoreA - predict).norm
    printf("norm of residuals %f\n", err)
    printf("train iteration rmses: %s\n", rmse)

    err should be < 15e-2

  }

}