<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 15px; border-radius: 5px; margin-bottom: 20px;"> <strong>⚠️ Deprecation Notice:</strong> This component (<em>Map Reduce</em>) is no longer actively maintained. It remains accessible for historical reference but is not recommended for new development.  Please check out <a href="https://mahout.apache.org/quantum-computing-primer/" style="color: #0c5460; text-decoration: underline;">Qumat - Mahout's Quantum Computing Primer</a> for the latest innovations.</div>

<p><a name="TwentyNewsgroups-TwentyNewsgroupsClassificationExample"></a></p>
<h2 id="twenty-newsgroups-classification-example">Twenty Newsgroups Classification Example</h2>

<p><a name="TwentyNewsgroups-Introduction"></a></p>
<h2 id="introduction">Introduction</h2>

<p>The 20 newsgroups dataset is a collection of approximately 20,000
newsgroup documents, partitioned (nearly) evenly across 20 different
newsgroups. The 20 newsgroups collection has become a popular data set for
experiments in text applications of machine learning techniques, such as
text classification and text clustering. We will use the <a href="http://mahout.apache.org/users/mapreduce/classification/bayesian.html">Mahout CBayes</a>
classifier to create a model that would classify a new document into one of
the 20 newsgroups.</p>

<p><a name="TwentyNewsgroups-Prerequisites"></a></p>
<h3 id="prerequisites">Prerequisites</h3>

<ul>
  <li>Mahout has been downloaded (<a href="https://mahout.apache.org/download/downloads.html">instructions here</a>)</li>
  <li>Maven is available</li>
  <li>Your environment has the following variables:
    <ul>
      <li><strong>HADOOP_HOME</strong> Environment variables refers to where Hadoop lives</li>
      <li><strong>MAHOUT_HOME</strong> Environment variables refers to where Mahout lives</li>
    </ul>
  </li>
</ul>

<p><a name="TwentyNewsgroups-Instructionsforrunningtheexample"></a></p>
<h3 id="instructions-for-running-the-example">Instructions for running the example</h3>

<ol>
  <li>
    <p>If running Hadoop in cluster mode, start the hadoop daemons by executing the following commands:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ cd $HADOOP_HOME/bin
     $ ./start-all.sh
</code></pre></div>    </div>

    <p>Otherwise:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ export MAHOUT_LOCAL=true
</code></pre></div>    </div>
  </li>
  <li>
    <p>In the trunk directory of Mahout, compile and install Mahout:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ cd $MAHOUT_HOME
     $ mvn -DskipTests clean install
</code></pre></div>    </div>
  </li>
  <li>
    <p>Run the <a href="https://github.com/apache/mahout/blob/master/examples/bin/classify-20newsgroups.sh">20 newsgroups example script</a> by executing:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ ./examples/bin/classify-20newsgroups.sh
</code></pre></div>    </div>
  </li>
  <li>
    <p>You will be prompted to select a classification method algorithm:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     1. Complement Naive Bayes
     2. Naive Bayes
     3. Stochastic Gradient Descent
</code></pre></div>    </div>
  </li>
</ol>

<p>Select 1 and the the script will perform the following:</p>

<ol>
  <li>Create a working directory for the dataset and all input/output.</li>
  <li>Download and extract the <em>20news-bydate.tar.gz</em> from the <a href="http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz">20 newsgroups dataset</a> to the working directory.</li>
  <li>Convert the full 20 newsgroups dataset into a &lt; Text, Text &gt; SequenceFile.</li>
  <li>Convert and preprocesses the dataset into a &lt; Text, VectorWritable &gt; SequenceFile containing term frequencies for each document.</li>
  <li>Split the preprocessed dataset into training and testing sets.</li>
  <li>Train the classifier.</li>
  <li>Test the classifier.</li>
</ol>

<p>Output should look something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=======================================================
Confusion Matrix
-------------------------------------------------------
 a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t &lt;--Classified as
381 0  0  0  0  9  1  0  0  0  1  0  0  2  0  1  0  0  3  0 |398 a=rec.motorcycles
 1 284 0  0  0  0  1  0  6  3  11 0  66 3  0  6  0  4  9  0 |395 b=comp.windows.x
 2  0 339 2  0  3  5  1  0  0  0  0  1  1  12 1  7  0  2  0 |376 c=talk.politics.mideast
 4  0  1 327 0  2  2  0  0  2  1  1  0  5  1  4  12 0  2  0 |364 d=talk.politics.guns
 7  0  4  32 27 7  7  2  0  12 0  0  6  0 100 9  7  31 0  0 |251 e=talk.religion.misc
 10 0  0  0  0 359 2  2  0  0  3  0  1  6  0  1  0  0  11 0 |396 f=rec.autos
 0  0  0  0  0  1 383 9  1  0  0  0  0  0  0  0  0  3  0  0 |397 g=rec.sport.baseball
 1  0  0  0  0  0  9 382 0  0  0  0  1  1  1  0  2  0  2  0 |399 h=rec.sport.hockey
 2  0  0  0  0  4  3  0 330 4  4  0  5  12 0  0  2  0  12 7 |385 i=comp.sys.mac.hardware
 0  3  0  0  0  0  1  0  0 368 0  0  10 4  1  3  2  0  2  0 |394 j=sci.space
 0  0  0  0  0  3  1  0  27 2 291 0  11 25 0  0  1  0  13 18|392 k=comp.sys.ibm.pc.hardware
 8  0  1 109 0  6  11 4  1  18 0  98 1  3  11 10 27 1  1  0 |310 l=talk.politics.misc
 0  11 0  0  0  3  6  0  10 6  11 0 299 13 0  2  13 0  7  8 |389 m=comp.graphics
 6  0  1  0  0  4  2  0  5  2  12 0  8 321 0  4  14 0  8  6 |393 n=sci.electronics
 2  0  0  0  0  0  4  1  0  3  1  0  3  1 372 6  0  2  1  2 |398 o=soc.religion.christian
 4  0  0  1  0  2  3  3  0  4  2  0  7  12 6 342 1  0  9  0 |396 p=sci.med
 0  1  0  1  0  1  4  0  3  0  1  0  8  4  0  2 369 0  1  1 |396 q=sci.crypt
 10 0  4  10 1  5  6  2  2  6  2  0  2  1 86 15 14 152 0  1 |319 r=alt.atheism
 4  0  0  0  0  9  1  1  8  1  12 0  3  0  2  0  0  0 341 2 |390 s=misc.forsale
 8  5  0  0  0  1  6  0  8  5  50 0  40 2  1  0  9  0  3 256|394 t=comp.os.ms-windows.misc
=======================================================
Statistics
-------------------------------------------------------
Kappa                                       0.8808
Accuracy                                   90.8596%
Reliability                                86.3632%
Reliability (standard deviation)            0.2131
</code></pre></div></div>

<p><a name="TwentyNewsgroups-ComplementaryNaiveBayes"></a></p>
<h2 id="end-to-end-commands-to-build-a-cbayes-model-for-20-newsgroups">End to end commands to build a CBayes model for 20 newsgroups</h2>
<p>The <a href="https://github.com/apache/mahout/blob/master/examples/bin/classify-20newsgroups.sh">20 newsgroups example script</a> issues the following commands as outlined above. We can build a CBayes classifier from the command line by following the process in the script:</p>

<p><em>Be sure that <strong>MAHOUT_HOME</strong>/bin and <strong>HADOOP_HOME</strong>/bin are in your <strong>$PATH</strong></em></p>

<ol>
  <li>
    <p>Create a working directory for the dataset and all input/output.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ export WORK_DIR=/tmp/mahout-work-${USER}
     $ mkdir -p ${WORK_DIR}
</code></pre></div>    </div>
  </li>
  <li>
    <p>Download and extract the <em>20news-bydate.tar.gz</em> from the <a href="http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz">20newsgroups dataset</a> to the working directory.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ curl http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz 
         -o ${WORK_DIR}/20news-bydate.tar.gz
     $ mkdir -p ${WORK_DIR}/20news-bydate
     $ cd ${WORK_DIR}/20news-bydate &amp;&amp; tar xzf ../20news-bydate.tar.gz &amp;&amp; cd .. &amp;&amp; cd ..
     $ mkdir ${WORK_DIR}/20news-all
     $ cp -R ${WORK_DIR}/20news-bydate/*/* ${WORK_DIR}/20news-all   * If you're running on a Hadoop cluster:
 
     $ hadoop dfs -put ${WORK_DIR}/20news-all ${WORK_DIR}/20news-all
</code></pre></div>    </div>
  </li>
  <li>
    <p>Convert the full 20 newsgroups dataset into a &lt; Text, Text &gt; SequenceFile.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ mahout seqdirectory 
         -i ${WORK_DIR}/20news-all 
         -o ${WORK_DIR}/20news-seq 
         -ow
</code></pre></div>    </div>
  </li>
  <li>
    <p>Convert and preprocesses the dataset into  a &lt; Text, VectorWritable &gt; SequenceFile containing term frequencies for each document.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ mahout seq2sparse 
         -i ${WORK_DIR}/20news-seq 
         -o ${WORK_DIR}/20news-vectors
         -lnorm 
         -nv 
         -wt tfidf If we wanted to use different parsing methods or transformations on the term frequency vectors we could supply different options here e.g.: -ng 2 for bigrams or -n 2 for L2 length normalization.  See the [Creating vectors from text](http://mahout.apache.org/users/basics/creating-vectors-from-text.html) page for a list of all seq2sparse options.   
</code></pre></div>    </div>
  </li>
  <li>
    <p>Split the preprocessed dataset into training and testing sets.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ mahout split 
         -i ${WORK_DIR}/20news-vectors/tfidf-vectors 
         --trainingOutput ${WORK_DIR}/20news-train-vectors 
         --testOutput ${WORK_DIR}/20news-test-vectors  
         --randomSelectionPct 40 
         --overwrite --sequenceFiles -xm sequential
</code></pre></div>    </div>
  </li>
  <li>
    <p>Train the classifier.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ mahout trainnb 
         -i ${WORK_DIR}/20news-train-vectors
         -el  
         -o ${WORK_DIR}/model 
         -li ${WORK_DIR}/labelindex 
         -ow 
         -c
</code></pre></div>    </div>
  </li>
  <li>
    <p>Test the classifier.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ mahout testnb 
         -i ${WORK_DIR}/20news-test-vectors
         -m ${WORK_DIR}/model 
         -l ${WORK_DIR}/labelindex 
         -ow 
         -o ${WORK_DIR}/20news-testing 
         -c
</code></pre></div>    </div>
  </li>
</ol>

