{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y5xLkFQ4sLOV",
        "outputId": "b3b21a29-a232-4cf0-ef94-4b2be060f48b"
      },
      "outputs": [],
      "source": [
        "%pip install qumat[qdp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvmpEUJFscx-",
        "outputId": "0c70d8eb-c7b4-4a87-914f-95c01d3fb26c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "QDP + QML: Full GPU Pipeline (float64)\n",
        "CPU → GPU (QDP batch encode) → GPU (real projection) → GPU (QML training)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from qumat.qdp import QdpEngine\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1. Setup\n",
        "# ─────────────────────────────────────────────\n",
        "DEVICE_ID = 0\n",
        "TORCH_DEVICE = torch.device(\"cuda\", DEVICE_ID)\n",
        "NUM_QUBITS = 2\n",
        "EPOCHS = 60\n",
        "LR = 0.01\n",
        "\n",
        "engine = QdpEngine(DEVICE_ID)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 2. Raw Data on CPU — float64\n",
        "# ─────────────────────────────────────────────\n",
        "raw = torch.tensor([\n",
        "    [0.5, 0.5, 0.5, 0.5],\n",
        "    [0.7, 0.1, 0.5, 0.3],\n",
        "    [0.1, 0.8, 0.4, 0.4],\n",
        "    [0.6, 0.2, 0.6, 0.4],\n",
        "], dtype=torch.float64)   # ← float64\n",
        "\n",
        "labels = torch.tensor([0, 1, 0, 1], dtype=torch.float64, device=TORCH_DEVICE)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 3. CPU → GPU: QDP Batch Encode\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"CPU → GPU: Batch encoding with QDP...\")\n",
        "cuda_batch = raw.cuda()\n",
        "\n",
        "qtensor = engine.encode(cuda_batch, num_qubits=NUM_QUBITS, encoding_method=\"amplitude\")\n",
        "\n",
        "# DLPack → complex128 CUDA tensor (two float64s per element)\n",
        "X_complex = torch.from_dlpack(qtensor)\n",
        "print(f\"Raw encoded: shape={X_complex.shape}, dtype={X_complex.dtype}, device={X_complex.device}\")\n",
        "\n",
        "# Concatenate real + imag → float64 [N, 8], stays on GPU\n",
        "X_quantum = torch.cat([X_complex.real, X_complex.imag], dim=-1).double()\n",
        "print(f\"Real features: shape={X_quantum.shape}, dtype={X_quantum.dtype}, device={X_quantum.device}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 4. QML Model on GPU — double precision\n",
        "# ─────────────────────────────────────────────\n",
        "class VariationalLayer(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.theta = nn.Parameter(torch.randn(dim, dtype=torch.float64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.cos(self.theta) + torch.roll(x, 1, dims=-1) * torch.sin(self.theta)\n",
        "\n",
        "class QMLClassifier(nn.Module):\n",
        "    def __init__(self, num_qubits):\n",
        "        super().__init__()\n",
        "        dim = 2 * (2 ** num_qubits)   # real + imag\n",
        "        self.layer1 = VariationalLayer(dim)\n",
        "        self.layer2 = VariationalLayer(dim)\n",
        "        self.readout = nn.Linear(dim, 1, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return torch.sigmoid(self.readout(x)).squeeze(-1)\n",
        "\n",
        "model = QMLClassifier(NUM_QUBITS).to(TORCH_DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 5. GPU Training\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\nGPU → Training QML model...\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(X_quantum)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            acc = ((preds > 0.5).double() == labels).double().mean().item()\n",
        "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f} | Accuracy: {acc:.2f}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 6. Inference\n",
        "# ─────────────────────────────────────────────\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted = (model(X_quantum) > 0.5).int()\n",
        "\n",
        "print(\"\\n─── Results ───\")\n",
        "for i, (pred, true) in enumerate(zip(predicted.cpu().tolist(), labels.int().cpu().tolist())):\n",
        "    print(f\"Sample {i}: Predicted={pred}  True={true}  {'✓' if pred == true else '✗'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
